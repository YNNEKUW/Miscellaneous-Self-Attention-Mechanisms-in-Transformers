# Miscellaneous-Self-Attention-Mechanisms-in-Transformers

> Here are several new attention mechanisms proposed to accelerate training. These algorithms can be catagorized into four types.
- Sparse Attention
- LSH
- Routing Transformer
- Synthesizer
