# Miscellaneous-Self-Attention-Mechanisms-in-Transformers

> We survey and implement several new attention mechanisms for transformer-based models, which are proposed to accelerate training.

**The algorithms we implement can be catagorized into four types:**
- Sparse Attention
- LSH
- Routing Transformer
- Synthesizer


## Table of Contents
 - [Introduction](#introduction)
 - [Example](#example)
 - [FAQ](#faq)
 
## Introduction 
ff

## Example
ff


## FAQ
ff

